{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, re, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_four_five_cal(date):\n",
    "    \"\"\"Generates four-four-five calendar based on date given as argument.\n",
    "       Enter first Monday of the fiscal year's first month.\"\"\"\n",
    "    \n",
    "    start_date = pd.Timestamp(date)\n",
    "    idx = pd.MultiIndex(levels=[[],[]], codes=[[],[]], names=['month', 'date'])\n",
    "    for i in list(range(1, 13)):\n",
    "        if i % 3 != 0:\n",
    "            weeks_in_month = 4\n",
    "            ts = pd.date_range(start=start_date, periods=(7 * weeks_in_month), freq='d')\n",
    "            update_idx = pd.MultiIndex.from_arrays([['month_' + str(i).zfill(2)] * (7 * weeks_in_month), ts])\n",
    "        else:\n",
    "            weeks_in_month = 5\n",
    "            ts = pd.date_range(start=start_date, periods=(7 * weeks_in_month), freq='d')\n",
    "            update_idx = pd.MultiIndex.from_arrays([['month_' + str(i).zfill(2)] * (7 * weeks_in_month), ts])\n",
    "        idx = idx.append(update_idx)\n",
    "        start_date = ts[-1] + pd.Timedelta(\"1 days\")\n",
    "        \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_daily_trans(file_name, usecols):\n",
    "    \"\"\"Load DailyTransActivity file and produce summary of activity by RP user ID.\"\"\"\n",
    "    \n",
    "    g01d = (pd.read_csv(file_name, dtype={'prtnum':object}, parse_dates=['trndte'], usecols=usecols)\n",
    "     .assign(trndte=lambda x: x.trndte - pd.Timedelta(hours=3))\n",
    "     .assign(previous_frstol=lambda x: x.groupby('usr_id')['frstol'].transform(lambda x: x.shift(1).fillna(method='bfill')))\n",
    "     .sort_values('trndte')\n",
    "    .reset_index(drop=True))\n",
    "    \n",
    "    process_dictionary = {\n",
    "        'Case Picking (FG)':g01d[(g01d['actcod'].isin(['CASPCK', 'LSTPCK'])) & (g01d['frstol'].apply(lambda x: re.search(digit_loc_regex, str(x)) is not None)) & (g01d['fr_arecod']).isin(['CAS020', 'PAL010', 'CAS010', 'CAS030', 'CAS040'])].index,\n",
    "        'Pallet Picking (FG)':g01d[(g01d['actcod'] == 'PALPCK') & (g01d['frstol'].apply(lambda x: re.search(digit_loc_regex, str(x)) is not None)) & (g01d['fr_arecod']).isin(['CAS020', 'PAL010', 'CAS010', 'CAS030', 'CAS040'])].index,\n",
    "        'Each Picking (SPA)':g01d[(g01d['actcod'].isin(['CASPCK', 'LSTPCK'])) & (g01d['frstol'].apply(lambda x: re.search(digit_loc_regex, str(x)) is not None)) & (g01d['fr_arecod']).isin(['CAS050', 'PCE010'])].index,\n",
    "        'Pick Deposit':g01d[(g01d['frstol'].apply(lambda x: re.search(rdt_loc_regex, str(x)) is None)) & (g01d['actcod'].isin(['CASPCK', 'LSTPCK', 'PALPCK']))].index,\n",
    "        'Packing (SPA)':g01d[(g01d['tostol'] == 'QA01') & (~g01d['ship_id'].isna())]['usr_id'].index,\n",
    "        'Receiving':g01d[(g01d['actcod'] == 'RCV') & (g01d['tostol'].apply(lambda x: re.search(rdt_loc_regex, str(x)) is None))].index,\n",
    "        'FG Putaway':g01d[(g01d['tostol'].apply(lambda x: re.search(digit_loc_regex, str(x)) is not None)) & (g01d['previous_frstol'].apply(lambda x: re.search(rec_loc_regex, str(x)) is not None)) & (g01d['to_arecod']).isin(['CAS020', 'PAL010', 'CAS010', 'CAS030', 'CAS040'])].index,\n",
    "        'SPA Putaway':g01d[(g01d['tostol'].apply(lambda x: re.search(digit_loc_regex, str(x)) is not None)) & (g01d['previous_frstol'].apply(lambda x: re.search(rec_loc_regex, str(x)) is not None)) & (g01d['to_arecod']).isin(['CAS050', 'PCE010'])].index\n",
    "        }\n",
    "    \n",
    "    g01d['process_name'] = pd.concat([pd.Series([k] * len(v), index=v, name='process')\n",
    "                                      for k, v in process_dictionary.items()]).sort_index()\n",
    "    \n",
    "    g01d['cases'] = g01d['trnqty'] / g01d['unit_per_cas']\n",
    "    \n",
    "    g01d['pallets'] = g01d['trnqty'] / g01d['unit_per_pal']\n",
    "    \n",
    "    rp_activity_summary = g01d.groupby(['usr_id', 'process_name'])[['trnqty', 'lodnum', 'cases', 'pallets']].agg({\n",
    "        'trnqty':sum,\n",
    "        'lodnum':'nunique',\n",
    "        'cases':sum,\n",
    "        'pallets':sum\n",
    "    }).unstack()\n",
    "    \n",
    "    rp_activity_summary.columns = rp_activity_summary.columns.map('{0[1]}|{0[0]}'.format)\n",
    "    \n",
    "    empty_df_cols = []\n",
    "    for i in list(process_dictionary.keys()):\n",
    "        for j in ['trnqty', 'lodnum', 'cases', 'pallets']:\n",
    "            empty_df_cols.append(i + '|' + j)\n",
    "            \n",
    "    rp_activity_summary = pd.concat([pd.DataFrame([], columns=empty_df_cols), rp_activity_summary], sort=False)\n",
    "    \n",
    "    rp_activity_summary['Packing (SPA)|lodnum'] = g01d[g01d['process_name'] == 'Packing (SPA)'].groupby('usr_id')['lodnum'].nunique()\n",
    "    \n",
    "    rp_activity_summary.index.name = 'rp_usr_id'\n",
    "    \n",
    "    rp_activity_summary.reset_index(inplace=True)\n",
    "    \n",
    "    rp_activity_summary.insert(0, 'date', (pd.Timestamp(file_name[-17:-7]) - pd.Timedelta('1 day')))\n",
    "    \n",
    "    return rp_activity_summary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usecols = ['dlytrn_id', 'trndte', 'oprcod', 'actcod', 'lodnum',\n",
    "       'prtnum', 'trnqty', 'traknm', 'fr_arecod', 'frstol',\n",
    "       'to_arecod', 'tostol', 'ship_id', 'trlr_num', 'usr_id', 'unit_per_cas', 'unit_per_pal', 'cas_per_pal']\n",
    "\n",
    "# RegEx Statements\n",
    "digit_loc_regex = re.compile('^\\d.*|COSTCO1')\n",
    "floor_loc_regex = re.compile('^\\d.*A')\n",
    "upper_loc_regex = re.compile('^\\d.*[B-Z]')\n",
    "rdt_loc_regex = re.compile('^((?!RDT).)*$')\n",
    "rec_loc_regex = re.compile('REC.*|\\d{10}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'file_path_prefix_here-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_ls = glob.glob(file_path + '*')\n",
    "file_ls = file_ls[-5:]\n",
    "file_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = four_four_five_cal(pd.Timestamp('2018-12-31'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trans_summary_df = pd.read_pickle(r'cumulative_pickle_file_path_here.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transaction Summary by 4-4-5 Calendar for Customer Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (pd.read_pickle(r'cumulative_pickle_file_path_here.pkl')\n",
    "      .drop(columns=['rp_usr_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_uom_2019 = (pd.DataFrame(index=idx).reset_index(level=1).merge(\n",
    "    df.fillna(0).groupby('date').sum(),\n",
    "    left_on='date',\n",
    "    right_index=True,\n",
    "    how='left')\n",
    " .fillna(0)).groupby('month').sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals_uom_2019.to_csv(r'output_path_here.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metric Received by 4-4-5 Calendar for Customer Budget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiving_df = pd.read_csv(r'output_path_here.csv', parse_dates=['dte'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiving_df = receiving_df.set_index(['dte', 'prtfam']).unstack(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiving_df.columns = receiving_df.columns.map('{0[1]}|{0[0]}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receiving_df[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_uom_for_budget_2019 = (pd.DataFrame(index=idx).reset_index(level=1).merge(\n",
    "    receiving_df,\n",
    "    left_on='date',\n",
    "    right_index=True,\n",
    "    how='left')\n",
    ".fillna(0)).groupby('month').sum().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "received_uom_for_budget_2019.to_csv(r'output_path_here.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_trans_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([summarize_daily_trans(file_ls[f], usecols=usecols) for f in list(range(len(file_ls)))], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_daily_trans_summary_df = pd.concat([daily_trans_summary_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_daily_trans_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_daily_trans_summary_df.to_pickle(r'output_file_path_here.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
