{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This script is run every morning to archive previous day's files\n",
    "## and update cumulative files used for analysis of trends and different time periods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil, time, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## I have a macro in Outlook running (in VBA folder of portfolio) which automatically downloads and file attachments, here.\n",
    "raw_reports_path = r'raw_reports_folder_path_here'\n",
    "pickle_path = r'path_to_share_drive_python_pickle_directory'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating Pickle Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punch Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading muliple Punch Summary Files\n",
    "df = pd.concat([(pd.read_excel(f, header=0, skiprows=8)\n",
    " .assign(total_hours=lambda x: x.groupby(['Employee Name', 'Date'])['Total Amount'].transform('sum'))\n",
    " .drop_duplicates(subset=['Employee Name', 'Date'], keep='last')) for f in glob.glob(raw_reports_path + '\\\\punch_summary_rolling' + '*.xls')], ignore_index=True)\n",
    "\n",
    "df['Total Amount'] = df.total_hours\n",
    "\n",
    "# This dataframe will be combined with previously saved data to have an updated cumulative file\n",
    "punch_summary_update = (df.drop_duplicates(subset=['Employee Name', 'Date'], keep='last')\n",
    " .sort_values(['Employee Name', 'Date'])\n",
    " .drop(columns=['total_hours']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punch_summary_cum = pd.read_pickle(pickle_path + '\\punch_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.concat([punch_summary_cum, punch_summary_update], ignore_index=True)\n",
    " .drop_duplicates(subset=['Date', 'Employee Name'], keep='last')\n",
    " .sort_values(['Date', 'Employee Name'])\n",
    " .reset_index(drop=True)).to_pickle(pickle_path + '\\punch_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Shipped\n",
    "\n",
    "metric_shipped_update = (pd.concat([pd.read_csv(f, dtype={'ordnum':object, 'prtnum':object},\n",
    "                           parse_dates=['dispatch_dte', 'order_add_dte'])\n",
    "            for f in glob.glob(raw_reports_path + '\\\\' + 'optredprairieTREKPRDleslogMetric-Shipped' + '*.csv')], ignore_index=True)\n",
    " .dropna(thresh=2)\n",
    " .sort_values(by='dispatch_dte')\n",
    " .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_shipped_cum = pd.read_pickle(pickle_path + '\\metric_shipped_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metric_shipped_reports = pd.concat([metric_shipped_cum, metric_shipped_update], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metric_shipped_reports.to_pickle(pickle_path + '\\metric_shipped_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Received\n",
    "\n",
    "metric_received_update = pd.concat([pd.read_csv(f,\n",
    "                                     dtype={'prtnum':object, 'prtfam':'category', 'invnum':object},\n",
    "                                     parse_dates=['trndte']) for f in glob.glob(raw_reports_path + '\\\\' + 'optredprairieTREKPRDleslogMetric-Received' + '*.csv')])\n",
    "\n",
    "metric_received_update.sort_values(by='trndte', inplace=True)\n",
    "metric_received_update.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_rec_cum = pd.read_pickle(pickle_path + '\\metric_received_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metric_received_reports = pd.concat([metric_rec_cum, metric_received_update], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_metric_received_reports.to_pickle(pickle_path + '\\metric_received_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Picking Summary\n",
    "\n",
    "picking_summary_update = pd.concat([pd.read_csv(f, dtype={'prt_fam': 'category'}, parse_dates=['pick_date']) for f in glob.glob(raw_reports_path + '\\\\' + 'optredprairieTREKPRDleslogDailyPickData' + '*.csv')])\n",
    "\n",
    "picking_summary_cum = pd.read_pickle(pickle_path + '\\picking_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_picking_summary_reports = pd.concat([picking_summary_cum, picking_summary_update], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_picking_summary_reports.to_pickle(pickle_path + '\\picking_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPA Carton Packing Summary\n",
    "\n",
    "spa_pack_summary_update = pd.concat([pd.read_csv(f, dtype=({'ordnum':object, 'traknm':object}),\n",
    "                      parse_dates=['adddte', 'cmpdte', 'prtdte', 'arcdte', 'moddte']) for f in glob.glob(raw_reports_path + '\\\\' + 'optredprairieTREKPRDleslogSPAPACKDTL' + '*.csv')])\n",
    "\n",
    "spa_pack_summary_cum = pd.read_pickle(pickle_path + '\\spa_pack_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_spa_pack_summary_reports = pd.concat([spa_pack_summary_cum, spa_pack_summary_update], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_spa_pack_summary_reports.to_pickle(pickle_path + '\\spa_pack_summary_cum.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Version of Processing Raw Files\n",
    "# The file names are already clean\n",
    "# All this script needs to do is move the files up through 4 AM of the current day to the archive folders.\n",
    "# Use Windows Task Scheduler to schedule this task to run every day before 8 AM\n",
    "#     (so I can start running hourly reporting at 8 AM)\n",
    "#\n",
    "# 9/4/2019 9:17 AM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime, re, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_archive_path(raw_file_path):\n",
    "    \"\"\"Create file path to archive items in raw_reports folder\"\"\"\n",
    "    \n",
    "    subfolder_search_result = re.search(subfolder_regex, raw_file_path)\n",
    "    filename_search_result = re.search(filename_regex, raw_file_path)\n",
    "    \n",
    "    if raw_file_path.endswith('.csv'):\n",
    "        return (r'my_username_path\\Data\\UC_RPT_EMAIL' +\n",
    "                '\\\\' +\n",
    "                raw_file_path[subfolder_search_result.start():subfolder_search_result.end()] +\n",
    "                '\\\\' +\n",
    "                raw_file_path[filename_search_result.start():filename_search_result.end()])\n",
    "    elif raw_file_path.endswith('.xls'):\n",
    "        return (r'my_username_path\\Data\\timekeeping_reports' +\n",
    "                '\\\\' +\n",
    "                raw_file_path[subfolder_search_result.start():subfolder_search_result.end()] +\n",
    "                '\\\\' +\n",
    "                raw_file_path[filename_search_result.start():filename_search_result.end()])\n",
    "    else:\n",
    "        return raw_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined Regular Expressions\n",
    "# https://regex101.com/\n",
    "# https://www.regular-expressions.info/\n",
    "\n",
    "subfolder_regex = re.compile('(?<=raw_reports\\\\\\\\)(.*?)(?=(-|_)\\d)')\n",
    "filename_regex = re.compile('(?<=raw_reports\\\\\\\\).*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current Date at 4:30 AM. Archive files before this time.\n",
    "# Leave remaining files to run hourly productivity report.\n",
    "\n",
    "datetime_cutoff = datetime.datetime.combine(datetime.datetime.now().date(), datetime.time(4, 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to unprocessed reports saved from email\n",
    "\n",
    "raw_reports_folder_path = r'my_username_path\\Downloads\\raw_reports'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files in raw_reports folder\n",
    "\n",
    "raw_reports_folder_files = [raw_reports_folder_path +\n",
    "                            '\\\\' +\n",
    "                            os.listdir(raw_reports_folder_path)[f]\n",
    "                            for f in list(range(len(os.listdir(raw_reports_folder_path))))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of files before datetime_cutoff to be fed to shutil.move function\n",
    "\n",
    "files_to_move = [raw_reports_folder_files[f] for f in list(range(len(raw_reports_folder_files)))\n",
    "                 if datetime.datetime.strptime(time.ctime(os.path.getmtime(raw_reports_folder_files[f])), '%a %b %d %H:%M:%S %Y')\n",
    "                 < datetime_cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving files in files_to_move list to archive locations\n",
    "# using shutil.move and get_file_archive_path functions.\n",
    "\n",
    "[shutil.move(files_to_move[f],\n",
    "       get_file_archive_path(files_to_move[f]))\n",
    "        for f in list(range(len(files_to_move)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a copy of the Roster from the share drive\n",
    "\n",
    "roster_file_path = glob.glob(r'share_drive_path\\Daily Attendance\\Daily Attendance' + '*.xlsx')[0]\n",
    "roster_destination_path = r'my_username_path\\Data\\timekeeping_reports\\roster\\Daily_Attendance_'\n",
    "\n",
    "if pd.Timestamp.now().weekday() in [1, 2, 3, 4, 5, 6]:\n",
    "    roster_file_name_date = (pd.Timestamp.now().date() - pd.Timedelta('1 day')).strftime('%y%m%d')\n",
    "else:\n",
    "    roster_file_name_date = (pd.Timestamp.now().date() - pd.Timedelta('3 day')).strftime('%y%m%d')\n",
    "\n",
    "shutil.copy2(roster_file_path, roster_destination_path + roster_file_name_date + '.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
